{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Link Prediction using Graph Neural Networks\n",
    "===========================================\n",
    "\n",
    "In the :doc:`introduction <1_introduction>`, you have already learned\n",
    "the basic workflow of using GNNs for node classification,\n",
    "i.e. predicting the category of a node in a graph. This tutorial will\n",
    "teach you how to train a GNN for link prediction, i.e. predicting the\n",
    "existence of an edge between two arbitrary nodes in a graph.\n",
    "\n",
    "By the end of this tutorial you will be able to\n",
    "\n",
    "-  Build a GNN-based link prediction model.\n",
    "-  Train and evaluate the model on a small DGL-provided dataset.\n",
    "\n",
    "(Time estimate: 28 minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('citeseer.content', 'r') as f:\n",
    "    feat_new = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('citeseer.cites', 'r') as f:\n",
    "    cites_new = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_new = {item.split('\\t')[0]: _ for _, item in enumerate(feat_new)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_neww = []\n",
    "for item in unlabeled_edges_list:\n",
    "    it = item['source'], item['target']\n",
    "    if it in cites_new_ or (it[1], it[0]) in cites_new_:\n",
    "        li_neww.append(1)\n",
    "    else:\n",
    "        li_neww.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "cites_new_ = [(features_new.get(item.strip().split('\\t')[0], -1), features_new.get(item.strip().split('\\t')[1], -1)) for item in cites_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Link Prediction with GNN\n",
    "------------------------------------\n",
    "\n",
    "Many applications such as social recommendation, item recommendation,\n",
    "knowledge graph completion, etc., can be formulated as link prediction,\n",
    "which predicts whether an edge exists between two particular nodes. This\n",
    "tutorial shows an example of predicting whether a citation relationship,\n",
    "either citing or being cited, between two papers exists in a citation\n",
    "network.\n",
    "\n",
    "This tutorial follows a relatively simple practice from\n",
    "`SEAL <https://papers.nips.cc/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf>`__.\n",
    "It formulates the link prediction problem as a binary classification\n",
    "problem as follows:\n",
    "\n",
    "-  Treat the edges in the graph as *positive examples*.\n",
    "-  Sample a number of non-existent edges (i.e. node pairs with no edges\n",
    "   between them) as *negative* examples.\n",
    "-  Divide the positive examples and negative examples into a training\n",
    "   set and a test set.\n",
    "-  Evaluate the model with any binary classification metric such as Area\n",
    "   Under Curve (AUC).\n",
    "\n",
    "In some domains such as large-scale recommender systems or information\n",
    "retrieval, you may favor metrics that emphasize good performance of\n",
    "top-K predictions. In these cases you may want to consider other metrics\n",
    "such as mean average precision, and use other negative sampling methods,\n",
    "which are beyond the scope of this tutorial.\n",
    "\n",
    "Loading graph and features\n",
    "--------------------------\n",
    "\n",
    "Following the :doc:`introduction <1_introduction>`, this tutorial\n",
    "first loads the Cora dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training and testing sets\n",
    "---------------------------------\n",
    "\n",
    "This tutorial randomly picks 10% of the edges for positive examples in\n",
    "the test set, and leave the rest for the training set. It then samples\n",
    "the same number of edges for negative examples in both sets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import dgl\n",
    "with open('labeled_edges.txt', 'r') as f:\n",
    "    labeled_edges = f.readlines()\n",
    "with open('unlabeled_edges.txt', 'r') as f:\n",
    "    unlabeled_edges = f.readlines()\n",
    "with open('features.txt', 'r') as f:\n",
    "    features = f.readlines()\n",
    "# features_full = [{'node': int(line[0]), 'features': ' '.join(line[1:])} for line in [line.strip().split() for line in features]]\n",
    "unlabeled_edges_list = [{'source': int(line[0]), 'target': int(line[1])} for line in [line.strip().split() for line in unlabeled_edges]]\n",
    "labeled_edges_list = [{'source': int(line[0]), 'target': int(line[1]), 'label': int(line[2])} for line in [line.strip().split() for line in labeled_edges]]\n",
    "# nodes = [item['node'] for item in features_full]\n",
    "features = [list(map(int, item.split())) for item in features]\n",
    "features = np.array([list(map(float, item)) for item in features])\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(nodes)\n",
    "# G.add_weighted_edges_from([(item['source'], item['target'], item['label']) for item in labeled_edges_list])\n",
    "# G.add_weighted_edges_from([(item['target'], item['source'], item['label']) for item in labeled_edges_list])\n",
    "\n",
    "# src = np.random.randint(0, 100, 500)\n",
    "# dst = np.random.randint(0, 100, 500)\n",
    "src = np.array([item['source'] for item in labeled_edges_list if item['label'] == 1])\n",
    "dst = np.array([item['target'] for item in labeled_edges_list if item['label'] == 1])\n",
    "src2 = np.array([item['source'] for item in labeled_edges_list if item['label'] == 0])\n",
    "dst2 = np.array([item['target'] for item in labeled_edges_list if item['label'] == 0])\n",
    "label = np.array([item['label'] for item in labeled_edges_list])\n",
    "# src2 = np.array([item['source'] for item in unlabeled_edges_list])\n",
    "# dst2 = np.array([item['target'] for item in unlabeled_edges_list])\n",
    "# make it symmetric\n",
    "\n",
    "edge_pred_graph = dgl.graph((np.concatenate([src, dst]), np.concatenate([dst, src])))\n",
    "# # synthetic node and edge features, as well as edge labels\n",
    "edge_pred_graph.ndata['feat'] = torch.FloatTensor(features)\n",
    "# edge_pred_graph.edata['feature'] = torch.randn(1000, 10)\n",
    "# edge_pred_graph.edata['label'] = torch.tensor(np.concatenate([label, label]))\n",
    "# # synthetic train-validation-test splits\n",
    "# edge_pred_graph.edata['train_mask'] = torch.zeros(2 * label.shape[0], dtype=torch.bool).bernoulli(0.6)\n",
    "\n",
    "g = edge_pred_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('features.txt', 'r') as f:\n",
    "#     features = f.readlines()\n",
    "# features_full = [{'node': int(line[0]), 'features': ' '.join(line[1:])} for line in [line.strip().split() for line in features]]\n",
    "# nodes = [item['node'] for item in features_full]\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(nodes)\n",
    "# G.add_weighted_edges_from([(item['source'], item['target'], item['label']) for item in labeled_edges_list])\n",
    "# G.add_weighted_edges_from([(item['target'], item['source'], item['label']) for item in labeled_edges_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split edge set for training and testing\n",
    "# u1 = []\n",
    "# v1 = []\n",
    "# for edge in G.edges():\n",
    "#     u1.append(int(edge[0]))\n",
    "#     v1.append(int(edge[1]))\n",
    "#     u1.append(int(edge[1]))\n",
    "#     v1.append(int(edge[0]))\n",
    "# # for edge in G.edges():\n",
    "# u1 = torch.tensor(u1, dtype=torch.int32)\n",
    "# v1 = torch.tensor(v1, dtype=torch.int32)\n",
    "    \n",
    "\n",
    "# eids = np.arange(2 * len(G.edges()))\n",
    "# eids = np.random.permutation(eids)\n",
    "# test_size = int(len(eids) * 0.1)\n",
    "# train_size = 2 * len(G.edges()) - test_size\n",
    "# test_pos_u, test_pos_v = u1[eids[:test_size]], v1[eids[:test_size]]\n",
    "# train_pos_u, train_pos_v = u1[eids[test_size:]], v1[eids[test_size:]]\n",
    "\n",
    "# # Find all negative edges and split them for training and testing\n",
    "# adj = sp.coo_matrix((np.ones(len(u1)), (u1.numpy(), v1.numpy())))\n",
    "# adj_neg = 1 - adj.todense() - np.eye(len(G.nodes()))\n",
    "# neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "# neg_eids = np.random.choice(len(neg_u), 2 * len(G.edges()) // 2)\n",
    "# test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "# train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = np.array([item['source'] for item in labeled_edges_list])\n",
    "# dst = np.array([item['target'] for item in labeled_edges_list])\n",
    "# label = np.array([item['label'] for item in labeled_edges_list])\n",
    "# # src2 = np.array([item['source'] for item in unlabeled_edges_list])\n",
    "# # dst2 = np.array([item['target'] for item in unlabeled_edges_list])\n",
    "# # make it symmetric\n",
    "\n",
    "# edge_pred_graph = dgl.graph((np.concatenate([src, dst]), np.concatenate([dst, src])))\n",
    "# # # synthetic node and edge features, as well as edge labels\n",
    "# edge_pred_graph.ndata['feature'] = torch.tensor(features)\n",
    "# # edge_pred_graph.edata['feature'] = torch.randn(1000, 10)\n",
    "# edge_pred_graph.edata['label'] = torch.tensor(np.concatenate([label, label]))\n",
    "# # # synthetic train-validation-test splits\n",
    "# edge_pred_graph.edata['train_mask'] = torch.zeros(2 * label.shape[0], dtype=torch.bool).bernoulli(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нужно запихнуть все положительные в pos а отрицательные в neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split edge set for training and testing\n",
    "u_, v_ = g.edges()\n",
    "u = u_#[:2*len(src)]\n",
    "v = v_#[:2*len(src)]\n",
    "\n",
    "eids = np.arange(2 * len(src))\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * 0.2)\n",
    "train_size = g.number_of_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "# train_pos_u, train_pos_v = u, v\n",
    "\n",
    "# Find all negative edges and split them for training and testing\n",
    "# adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "# adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "# neg_u, neg_v = np.where(adj_neg != 0)\n",
    "# neg_u = u_[2*len(src):]\n",
    "# neg_v = v_[2*len(src):]\n",
    "neg_u = src2\n",
    "neg_v = dst2\n",
    "\n",
    "# neg_eids = np.random.choice(len(neg_u), g.number_of_edges() // 2)\n",
    "test_neg_u, test_neg_v = neg_u[:test_size // 2], neg_v[: test_size // 2 ]\n",
    "train_neg_u, train_neg_v = neg_u[test_size // 2:], neg_v[test_size // 2:]\n",
    "# test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "# train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "\n",
    "# train_neg_u, train_neg_v = neg_u, neg_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3312"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, you will need to remove the edges in the test set from\n",
    "the original graph. You can do this via ``dgl.remove_edges``.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``dgl.remove_edges`` works by creating a subgraph from the\n",
    "   original graph, resulting in a copy and therefore could be slow for\n",
    "   large graphs. If so, you could save the training and test graph to\n",
    "   disk, as you would do for preprocessing.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in zip(train_pos_u, train_pos_v):\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7, 616) in G.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = dgl.remove_edges(g, eids[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_g, train_g.ndata['feat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a GraphSAGE model\n",
    "------------------------\n",
    "\n",
    "This tutorial builds a model consisting of two\n",
    "`GraphSAGE <https://arxiv.org/abs/1706.02216>`__ layers, each computes\n",
    "new node representations by averaging neighbor information. DGL provides\n",
    "``dgl.nn.SAGEConv`` that conveniently creates a GraphSAGE layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, dropout):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.r = nn.LeakyReLU(0.1)\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats*2, 'gcn')\n",
    "        self.conv2 = SAGEConv(h_feats*2, h_feats, 'gcn')\n",
    "        self.conv3 = SAGEConv(h_feats, h_feats, 'gcn')\n",
    "#         self.conv4 = SAGEConv(h_feats, h_feats, 'gcn')\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "    \n",
    "    def forward(self, g, in_feat, mode='eval'):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        if mode == 'train':\n",
    "            h = self.dropout(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(g, h)\n",
    "#         h = F.relu(h)\n",
    "#         h = self.conv4(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model then predicts the probability of existence of an edge by\n",
    "computing a score between the representations of both incident nodes\n",
    "with a function (e.g. an MLP or a dot product), which you will see in\n",
    "the next section.\n",
    "\n",
    "\\begin{align}\\hat{y}_{u\\sim v} = f(h_u, h_v)\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive graph, negative graph, and ``apply_edges``\n",
    "---------------------------------------------------\n",
    "\n",
    "In previous tutorials you have learned how to compute node\n",
    "representations with a GNN. However, link prediction requires you to\n",
    "compute representation of *pairs of nodes*.\n",
    "\n",
    "DGL recommends you to treat the pairs of nodes as another graph, since\n",
    "you can describe a pair of nodes with an edge. In link prediction, you\n",
    "will have a *positive graph* consisting of all the positive examples as\n",
    "edges, and a *negative graph* consisting of all the negative examples.\n",
    "The *positive graph* and the *negative graph* will contain the same set\n",
    "of nodes as the original graph.  This makes it easier to pass node\n",
    "features among multiple graphs for computation.  As you will see later,\n",
    "you can directly fed the node representations computed on the entire\n",
    "graph to the positive and the negative graphs for computing pair-wise\n",
    "scores.\n",
    "\n",
    "The following code constructs the positive graph and the negative graph\n",
    "for the training set and the test set respectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of treating the pairs of nodes as a graph is that you can\n",
    "use the ``DGLGraph.apply_edges`` method, which conveniently computes new\n",
    "edge features based on the incident nodes’ features and the original\n",
    "edge features (if applicable).\n",
    "\n",
    "DGL provides a set of optimized builtin functions to compute new\n",
    "edge features based on the original node/edge features. For example,\n",
    "``dgl.function.u_dot_v`` computes a dot product of the incident nodes’\n",
    "representations for each edge.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_new = torch.tensor([item['source'] for item in unlabeled_edges_list])\n",
    "dst_new = torch.tensor([item['target'] for item in unlabeled_edges_list])\n",
    "new_inf = dgl.graph((src_new, dst_new), num_nodes=g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dgl.function as fn\n",
    "\n",
    "# class DotPredictor(nn.Module):\n",
    "#     def forward(self, g, h):\n",
    "#         with g.local_scope():\n",
    "#             g.ndata['h'] = h\n",
    "#             # Compute a new edge feature named 'score' by a dot-product between the\n",
    "#             # source node feature 'h' and destination node feature 'h'.\n",
    "#             g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "#             # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "#             return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write your own function if it is complex.\n",
    "For instance, the following module produces a scalar score on each edge\n",
    "by concatenating the incident nodes’ features and passing it to an MLP.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        \"\"\"\n",
    "        Computes a scalar score for each edge of the given graph.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        edges :\n",
    "            Has three members ``src``, ``dst`` and ``data``, each of\n",
    "            which is a dictionary representing the features of the\n",
    "            source nodes, the destination nodes, and the edges\n",
    "            themselves.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary of new edge features.\n",
    "        \"\"\"\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        score = torch.sigmoid(self.W2(F.relu(self.W1(h))).squeeze(1))\n",
    "#         score = self.W2(F.relu(self.W1(h))).squeeze(1)\n",
    "        return {'score': score}\n",
    "\n",
    "    def forward(self, g, h, d=False):\n",
    "        with g.local_scope():\n",
    "#             if d:\n",
    "#                 h = nn.functional.dropout(h, p=0.1)\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, feat_in, h_feats, dropout):\n",
    "        super().__init__()\n",
    "        self.sage = GraphSAGE(feat_in, h_feats, dropout)\n",
    "        self.pred = MLPPredictor(h_feats)\n",
    "    \n",
    "    def forward(self, train_g, features, pos, neg, mode=\"eval\"):\n",
    "        h = self.sage(train_g, features, mode)\n",
    "        pos_score = self.pred(pos, h, True)\n",
    "        neg_score = self.pred(neg, h, True)\n",
    "        return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 5.697616100311279, val_loss: 5.299469947814941, AUC: 0.49667774086378735\n",
      "In epoch 1, loss: 23.22168731689453, val_loss: 22.32206153869629, AUC: 0.523671096345515\n",
      "In epoch 2, loss: 22.80771827697754, val_loss: 22.5015926361084, AUC: 0.5\n",
      "In epoch 3, loss: 11.896060943603516, val_loss: 10.815698623657227, AUC: 0.5\n",
      "In epoch 4, loss: 2.8950796127319336, val_loss: 2.74806547164917, AUC: 0.5\n",
      "In epoch 5, loss: 0.7994735836982727, val_loss: 0.7475866675376892, AUC: 0.5240863787375416\n",
      "In epoch 6, loss: 0.8651487827301025, val_loss: 0.8556873798370361, AUC: 0.48588039867109634\n",
      "In epoch 7, loss: 0.6579791903495789, val_loss: 0.6764896512031555, AUC: 0.5\n",
      "In epoch 8, loss: 0.7037294507026672, val_loss: 0.684185266494751, AUC: 0.5182724252491694\n",
      "In epoch 9, loss: 0.634511411190033, val_loss: 0.6363199353218079, AUC: 0.5477574750830565\n",
      "In epoch 10, loss: 0.6806454658508301, val_loss: 0.681261420249939, AUC: 0.5577242524916943\n",
      "In epoch 11, loss: 0.6348400115966797, val_loss: 0.6341421008110046, AUC: 0.5992524916943522\n",
      "In epoch 12, loss: 0.6253770589828491, val_loss: 0.6388352513313293, AUC: 0.5352990033222591\n",
      "In epoch 13, loss: 0.6539145708084106, val_loss: 0.6492171287536621, AUC: 0.5278239202657807\n",
      "In epoch 14, loss: 0.5881920456886292, val_loss: 0.6310593485832214, AUC: 0.5406976744186047\n",
      "In epoch 15, loss: 0.6201271414756775, val_loss: 0.6237421035766602, AUC: 0.5743355481727574\n",
      "In epoch 16, loss: 0.6277130842208862, val_loss: 0.6261923909187317, AUC: 0.6071428571428572\n",
      "In epoch 17, loss: 0.6224491000175476, val_loss: 0.6227728128433228, AUC: 0.6058970099667774\n",
      "In epoch 18, loss: 0.613163411617279, val_loss: 0.624614953994751, AUC: 0.574750830564784\n",
      "In epoch 19, loss: 0.6177058219909668, val_loss: 0.6244155168533325, AUC: 0.5490033222591363\n",
      "In epoch 20, loss: 0.6209343671798706, val_loss: 0.6227042078971863, AUC: 0.5406976744186047\n",
      "In epoch 21, loss: 0.6159397959709167, val_loss: 0.6178265810012817, AUC: 0.551079734219269\n",
      "In epoch 22, loss: 0.6010002493858337, val_loss: 0.6068413853645325, AUC: 0.5751661129568106\n",
      "In epoch 23, loss: 0.6011713147163391, val_loss: 0.6040340662002563, AUC: 0.6129568106312293\n",
      "In epoch 24, loss: 0.5829388499259949, val_loss: 0.5997649431228638, AUC: 0.6154485049833887\n",
      "In epoch 25, loss: 0.5860549211502075, val_loss: 0.5971762537956238, AUC: 0.5971760797342193\n",
      "In epoch 26, loss: 0.5770765542984009, val_loss: 0.5967304706573486, AUC: 0.5934385382059801\n",
      "In epoch 27, loss: 0.5760936141014099, val_loss: 0.5952911972999573, AUC: 0.5950996677740864\n",
      "In epoch 28, loss: 0.5657318830490112, val_loss: 0.589843213558197, AUC: 0.6054817275747508\n",
      "In epoch 29, loss: 0.5517539381980896, val_loss: 0.5879656076431274, AUC: 0.6150332225913622\n",
      "In epoch 30, loss: 0.559182345867157, val_loss: 0.5890749096870422, AUC: 0.625\n",
      "In epoch 31, loss: 0.5555158257484436, val_loss: 0.5864308476448059, AUC: 0.6237541528239203\n",
      "In epoch 32, loss: 0.5507981777191162, val_loss: 0.5833534598350525, AUC: 0.6179401993355482\n",
      "In epoch 33, loss: 0.5493930578231812, val_loss: 0.5831426978111267, AUC: 0.6175249169435215\n",
      "In epoch 34, loss: 0.546419084072113, val_loss: 0.5822452902793884, AUC: 0.6216777408637874\n",
      "In epoch 35, loss: 0.5335754156112671, val_loss: 0.5801189541816711, AUC: 0.6270764119601329\n",
      "In epoch 36, loss: 0.5350399017333984, val_loss: 0.5801995992660522, AUC: 0.6387043189368771\n",
      "In epoch 37, loss: 0.5282702445983887, val_loss: 0.5795911550521851, AUC: 0.6407807308970099\n",
      "In epoch 38, loss: 0.5115204453468323, val_loss: 0.5761021375656128, AUC: 0.6432724252491695\n",
      "In epoch 39, loss: 0.5207407474517822, val_loss: 0.5747857093811035, AUC: 0.6378737541528239\n",
      "In epoch 40, loss: 0.5159581899642944, val_loss: 0.5742355585098267, AUC: 0.6399501661129567\n",
      "In epoch 41, loss: 0.49777141213417053, val_loss: 0.5740360021591187, AUC: 0.6399501661129567\n",
      "In epoch 42, loss: 0.5103538036346436, val_loss: 0.5785523056983948, AUC: 0.6432724252491695\n",
      "In epoch 43, loss: 0.5103036761283875, val_loss: 0.574695885181427, AUC: 0.6407807308970099\n",
      "In epoch 44, loss: 0.5044107437133789, val_loss: 0.5719608068466187, AUC: 0.6349667774086378\n",
      "In epoch 45, loss: 0.5128722786903381, val_loss: 0.5670434832572937, AUC: 0.6416112956810631\n",
      "In epoch 46, loss: 0.5047435164451599, val_loss: 0.5639535188674927, AUC: 0.6449335548172758\n",
      "In epoch 47, loss: 0.5001761317253113, val_loss: 0.561965823173523, AUC: 0.6387043189368771\n",
      "In epoch 48, loss: 0.5027433037757874, val_loss: 0.5596325993537903, AUC: 0.6407807308970099\n",
      "In epoch 49, loss: 0.5034990310668945, val_loss: 0.5571449398994446, AUC: 0.643687707641196\n",
      "In epoch 50, loss: 0.4955533444881439, val_loss: 0.555498480796814, AUC: 0.6470099667774086\n",
      "In epoch 51, loss: 0.4875571131706238, val_loss: 0.5538824796676636, AUC: 0.6445182724252492\n",
      "In epoch 52, loss: 0.4607313871383667, val_loss: 0.5507750511169434, AUC: 0.6519933554817275\n",
      "In epoch 53, loss: 0.48174628615379333, val_loss: 0.5453335642814636, AUC: 0.6524086378737541\n",
      "In epoch 54, loss: 0.47067686915397644, val_loss: 0.5380121469497681, AUC: 0.6681893687707641\n",
      "In epoch 55, loss: 0.47968438267707825, val_loss: 0.5319375991821289, AUC: 0.673172757475083\n",
      "In epoch 56, loss: 0.47375962138175964, val_loss: 0.5260584354400635, AUC: 0.6727574750830565\n",
      "In epoch 57, loss: 0.46864503622055054, val_loss: 0.5194072723388672, AUC: 0.6748338870431894\n",
      "In epoch 58, loss: 0.46443670988082886, val_loss: 0.5121773481369019, AUC: 0.6798172757475083\n",
      "In epoch 59, loss: 0.46476349234580994, val_loss: 0.5053105354309082, AUC: 0.683970099667774\n",
      "In epoch 60, loss: 0.47176796197891235, val_loss: 0.49577370285987854, AUC: 0.6872923588039866\n",
      "In epoch 61, loss: 0.45361900329589844, val_loss: 0.49384814500808716, AUC: 0.6885382059800664\n",
      "In epoch 62, loss: 0.4480143189430237, val_loss: 0.489086776971817, AUC: 0.6864617940199336\n",
      "In epoch 63, loss: 0.45467108488082886, val_loss: 0.4846687614917755, AUC: 0.6885382059800664\n",
      "In epoch 64, loss: 0.4558526873588562, val_loss: 0.48005810379981995, AUC: 0.6918604651162791\n",
      "In epoch 65, loss: 0.44269025325775146, val_loss: 0.47612810134887695, AUC: 0.6964285714285714\n",
      "In epoch 66, loss: 0.43703362345695496, val_loss: 0.4726875424385071, AUC: 0.6939368770764119\n",
      "In epoch 67, loss: 0.43697869777679443, val_loss: 0.4710622727870941, AUC: 0.6993355481727574\n",
      "In epoch 68, loss: 0.4230199456214905, val_loss: 0.47068625688552856, AUC: 0.698920265780731\n",
      "In epoch 69, loss: 0.4380338788032532, val_loss: 0.4736420810222626, AUC: 0.6935215946843853\n",
      "In epoch 70, loss: 0.42076200246810913, val_loss: 0.4715166687965393, AUC: 0.6960132890365449\n",
      "In epoch 71, loss: 0.4238640367984772, val_loss: 0.46840277314186096, AUC: 0.696843853820598\n",
      "In epoch 72, loss: 0.4138282537460327, val_loss: 0.4678135812282562, AUC: 0.6960132890365449\n",
      "In epoch 73, loss: 0.4223521053791046, val_loss: 0.4680747985839844, AUC: 0.6993355481727574\n",
      "In epoch 74, loss: 0.4433043599128723, val_loss: 0.4689018428325653, AUC: 0.7001661129568106\n",
      "In epoch 75, loss: 0.4108666777610779, val_loss: 0.4709717035293579, AUC: 0.6980897009966778\n",
      "In epoch 76, loss: 0.408475399017334, val_loss: 0.47082850337028503, AUC: 0.6972591362126246\n",
      "In epoch 77, loss: 0.4006146788597107, val_loss: 0.46927404403686523, AUC: 0.7022425249169435\n",
      "In epoch 78, loss: 0.4032704532146454, val_loss: 0.46949830651283264, AUC: 0.7026578073089701\n",
      "In epoch 79, loss: 0.4005507230758667, val_loss: 0.47250261902809143, AUC: 0.7043189368770764\n",
      "In epoch 80, loss: 0.39540624618530273, val_loss: 0.4741794168949127, AUC: 0.7113787375415282\n",
      "In epoch 81, loss: 0.39896994829177856, val_loss: 0.48135507106781006, AUC: 0.7014119601328903\n",
      "In epoch 82, loss: 0.3964444100856781, val_loss: 0.4725125730037689, AUC: 0.7084717607973423\n",
      "In epoch 83, loss: 0.38735634088516235, val_loss: 0.4869738221168518, AUC: 0.7047342192691031\n",
      "In epoch 84, loss: 0.39172956347465515, val_loss: 0.49130234122276306, AUC: 0.704734219269103\n",
      "In epoch 85, loss: 0.3837507367134094, val_loss: 0.48045286536216736, AUC: 0.704734219269103\n",
      "In epoch 86, loss: 0.38290461897850037, val_loss: 0.4778035581111908, AUC: 0.7093023255813954\n",
      "In epoch 87, loss: 0.3792491555213928, val_loss: 0.484697550535202, AUC: 0.7063953488372093\n",
      "In epoch 88, loss: 0.372002512216568, val_loss: 0.4935925304889679, AUC: 0.7084717607973422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 89, loss: 0.37366679310798645, val_loss: 0.4904834032058716, AUC: 0.7109634551495017\n",
      "In epoch 90, loss: 0.3739759027957916, val_loss: 0.48571476340293884, AUC: 0.7101328903654485\n",
      "In epoch 91, loss: 0.3673158586025238, val_loss: 0.48788735270500183, AUC: 0.7138704318936877\n",
      "In epoch 92, loss: 0.370735764503479, val_loss: 0.4971090853214264, AUC: 0.7122093023255814\n",
      "In epoch 93, loss: 0.3752100169658661, val_loss: 0.5060659050941467, AUC: 0.7138704318936877\n",
      "In epoch 94, loss: 0.3593093454837799, val_loss: 0.49367672204971313, AUC: 0.715531561461794\n",
      "In epoch 95, loss: 0.3559504449367523, val_loss: 0.49130144715309143, AUC: 0.7180232558139534\n",
      "In epoch 96, loss: 0.3471277952194214, val_loss: 0.4977213442325592, AUC: 0.723421926910299\n",
      "In epoch 97, loss: 0.35855183005332947, val_loss: 0.5104701519012451, AUC: 0.7221760797342193\n",
      "In epoch 98, loss: 0.3627108037471771, val_loss: 0.5058494210243225, AUC: 0.7279900332225914\n",
      "In epoch 99, loss: 0.3632584512233734, val_loss: 0.5002349615097046, AUC: 0.7250830564784053\n",
      "In epoch 100, loss: 0.3462883532047272, val_loss: 0.5014962553977966, AUC: 0.7308970099667774\n",
      "In epoch 101, loss: 0.3441316485404968, val_loss: 0.5161997675895691, AUC: 0.7254983388704319\n",
      "In epoch 102, loss: 0.3503815531730652, val_loss: 0.518679141998291, AUC: 0.7246677740863787\n",
      "In epoch 103, loss: 0.34177449345588684, val_loss: 0.5010442733764648, AUC: 0.7275747508305649\n",
      "In epoch 104, loss: 0.3412785232067108, val_loss: 0.5057183504104614, AUC: 0.7225913621262459\n",
      "In epoch 105, loss: 0.3343901038169861, val_loss: 0.5092176198959351, AUC: 0.7304817275747508\n",
      "In epoch 106, loss: 0.33665817975997925, val_loss: 0.5292890071868896, AUC: 0.728405315614618\n",
      "In epoch 107, loss: 0.33402854204177856, val_loss: 0.5281874537467957, AUC: 0.7313122923588039\n",
      "In epoch 108, loss: 0.33095529675483704, val_loss: 0.5163756608963013, AUC: 0.7338039867109635\n",
      "In epoch 109, loss: 0.33574286103248596, val_loss: 0.5131152868270874, AUC: 0.7284053156146179\n",
      "In epoch 110, loss: 0.3302779495716095, val_loss: 0.5143090486526489, AUC: 0.7346345514950167\n",
      "In epoch 111, loss: 0.3240329325199127, val_loss: 0.5335667133331299, AUC: 0.7321428571428571\n",
      "In epoch 112, loss: 0.32060232758522034, val_loss: 0.5404160618782043, AUC: 0.7338039867109635\n",
      "In epoch 113, loss: 0.32400476932525635, val_loss: 0.5372927784919739, AUC: 0.7350498338870433\n",
      "In epoch 114, loss: 0.32930588722229004, val_loss: 0.5270591974258423, AUC: 0.7317275747508306\n",
      "In epoch 115, loss: 0.31800326704978943, val_loss: 0.5238202214241028, AUC: 0.7329734219269103\n",
      "In epoch 116, loss: 0.3163960874080658, val_loss: 0.5301311612129211, AUC: 0.7387873754152823\n",
      "In epoch 117, loss: 0.3156713843345642, val_loss: 0.5355224013328552, AUC: 0.7379568106312292\n",
      "In epoch 118, loss: 0.30758222937583923, val_loss: 0.5413004755973816, AUC: 0.7387873754152824\n",
      "In epoch 119, loss: 0.31063228845596313, val_loss: 0.5466246008872986, AUC: 0.737126245847176\n",
      "In epoch 120, loss: 0.3111065626144409, val_loss: 0.5445356369018555, AUC: 0.7387873754152825\n",
      "In epoch 121, loss: 0.3057297468185425, val_loss: 0.5443218946456909, AUC: 0.7387873754152825\n",
      "In epoch 122, loss: 0.30828872323036194, val_loss: 0.5429791212081909, AUC: 0.7408637873754153\n",
      "In epoch 123, loss: 0.31746718287467957, val_loss: 0.5462245345115662, AUC: 0.7371262458471761\n",
      "In epoch 124, loss: 0.2960060238838196, val_loss: 0.5507742762565613, AUC: 0.7392026578073089\n",
      "In epoch 125, loss: 0.3082887530326843, val_loss: 0.558488667011261, AUC: 0.741279069767442\n",
      "In epoch 126, loss: 0.299491822719574, val_loss: 0.5631893873214722, AUC: 0.7354651162790697\n",
      "In epoch 127, loss: 0.30206093192100525, val_loss: 0.5610082745552063, AUC: 0.7358803986710963\n",
      "In epoch 128, loss: 0.29514896869659424, val_loss: 0.5557121634483337, AUC: 0.7400332225913622\n",
      "In epoch 129, loss: 0.29283812642097473, val_loss: 0.5519326329231262, AUC: 0.7408637873754153\n",
      "In epoch 130, loss: 0.29374006390571594, val_loss: 0.5548694133758545, AUC: 0.7367109634551494\n",
      "In epoch 131, loss: 0.28693053126335144, val_loss: 0.5663248896598816, AUC: 0.7371262458471761\n",
      "In epoch 132, loss: 0.2963382303714752, val_loss: 0.5813050270080566, AUC: 0.741279069767442\n",
      "In epoch 133, loss: 0.31376782059669495, val_loss: 0.5784401893615723, AUC: 0.742109634551495\n",
      "In epoch 134, loss: 0.2867293655872345, val_loss: 0.5695801973342896, AUC: 0.7379568106312293\n",
      "In epoch 135, loss: 0.2915474474430084, val_loss: 0.5715180039405823, AUC: 0.7304817275747508\n",
      "In epoch 136, loss: 0.28197214007377625, val_loss: 0.5754620432853699, AUC: 0.7429401993355482\n",
      "In epoch 137, loss: 0.28951236605644226, val_loss: 0.5870100855827332, AUC: 0.7429401993355481\n",
      "In epoch 138, loss: 0.2803776264190674, val_loss: 0.589862048625946, AUC: 0.7362956810631229\n",
      "In epoch 139, loss: 0.28396719694137573, val_loss: 0.5906961560249329, AUC: 0.7321428571428571\n",
      "In epoch 140, loss: 0.2863227128982544, val_loss: 0.5864962339401245, AUC: 0.7408637873754154\n",
      "In epoch 141, loss: 0.2799912989139557, val_loss: 0.5918683409690857, AUC: 0.7429401993355481\n",
      "In epoch 142, loss: 0.28247320652008057, val_loss: 0.5982552170753479, AUC: 0.7358803986710963\n",
      "In epoch 143, loss: 0.2831163704395294, val_loss: 0.6026660799980164, AUC: 0.7396179401993356\n",
      "In epoch 144, loss: 0.27661123871803284, val_loss: 0.5993601083755493, AUC: 0.7416943521594683\n",
      "In epoch 145, loss: 0.27327898144721985, val_loss: 0.5990085005760193, AUC: 0.740033222591362\n",
      "In epoch 146, loss: 0.28513219952583313, val_loss: 0.604345977306366, AUC: 0.7333887043189369\n",
      "In epoch 147, loss: 0.27349263429641724, val_loss: 0.6065325140953064, AUC: 0.7367109634551495\n",
      "In epoch 148, loss: 0.27573034167289734, val_loss: 0.6123090386390686, AUC: 0.7425249169435215\n",
      "In epoch 149, loss: 0.27985382080078125, val_loss: 0.6172540187835693, AUC: 0.7408637873754153\n",
      "In epoch 150, loss: 0.26830801367759705, val_loss: 0.6181754469871521, AUC: 0.7400332225913621\n",
      "In epoch 151, loss: 0.27889275550842285, val_loss: 0.6130053400993347, AUC: 0.7383720930232558\n",
      "In epoch 152, loss: 0.27438902854919434, val_loss: 0.6132780313491821, AUC: 0.7404485049833887\n",
      "In epoch 153, loss: 0.26838019490242004, val_loss: 0.6130473613739014, AUC: 0.7367109634551494\n",
      "In epoch 154, loss: 0.2692594826221466, val_loss: 0.6193321943283081, AUC: 0.7425249169435215\n",
      "In epoch 155, loss: 0.26624512672424316, val_loss: 0.6141136288642883, AUC: 0.7400332225913621\n",
      "In epoch 156, loss: 0.278481662273407, val_loss: 0.6169192790985107, AUC: 0.744186046511628\n",
      "In epoch 157, loss: 0.2745470404624939, val_loss: 0.6391270756721497, AUC: 0.7404485049833888\n",
      "In epoch 158, loss: 0.27564582228660583, val_loss: 0.636447548866272, AUC: 0.7408637873754154\n",
      "In epoch 159, loss: 0.27592119574546814, val_loss: 0.6320124864578247, AUC: 0.7416943521594684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# in this case, loss will in training loop\n",
    "model = FullModel(train_g.ndata['feat'].shape[1], 16, 0.01)\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "best_val_loss = 1\n",
    "for e in range(160):\n",
    "    # forward\n",
    "    pos_score, neg_score = model(train_g, train_g.ndata['feat'], train_pos_g, train_neg_g, mode=\"train\")\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pos_score, neg_score = model(train_g, train_g.ndata['feat'], test_pos_g, test_neg_g)\n",
    "        val_loss = compute_loss(pos_score, neg_score)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_model = model\n",
    "            best_val_loss = val_loss\n",
    "#     if val_loss < 0.45 or compute_auc(pos_score, neg_score) > 0.75:\n",
    "#         print('In epoch {}, loss: {}, val_loss: {}, AUC: {}'.format(e, loss, val_loss, compute_auc(pos_score, neg_score)))\n",
    "#         best_model = model\n",
    "#         break\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#     if e % 1 == 4:\n",
    "#         print('In epoch {}, loss: {}'.format(e, loss))\n",
    "    print('In epoch {}, loss: {}, val_loss: {}, AUC: {}'.format(e, loss, val_loss, compute_auc(pos_score, neg_score)))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pos_score = pred(test_pos_g, h)\n",
    "#     neg_score = pred(test_neg_g, h)\n",
    "#     print('AUC', compute_auc(pos_score, neg_score), compute_loss(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The builtin functions are optimized for both speed and memory.\n",
    "   We recommend using builtin functions whenever possible.</p></div>\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>If you have read the :doc:`message passing\n",
    "   tutorial <3_message_passing>`, you will notice that the\n",
    "   argument ``apply_edges`` takes has exactly the same form as a message\n",
    "   function in ``update_all``.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "-------------\n",
    "\n",
    "After you defined the node representation computation and the edge score\n",
    "computation, you can go ahead and define the overall model, loss\n",
    "function, and evaluation metric.\n",
    "\n",
    "The loss function is simply binary cross entropy loss.\n",
    "\n",
    "\\begin{align}\\mathcal{L} = -\\sum_{u\\sim v\\in \\mathcal{D}}\\left( y_{u\\sim v}\\log(\\hat{y}_{u\\sim v}) + (1-y_{u\\sim v})\\log(1-\\hat{y}_{u\\sim v})) \\right)\\end{align}\n",
    "\n",
    "The evaluation metric in this tutorial is AUC.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "model = GraphSAGE(train_g.ndata['feat'].shape[1], 16, 0.3)\n",
    "# You can replace DotPredictor with MLPPredictor.\n",
    "pred = MLPPredictor(16)\n",
    "# pred = DotPredictor()\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "#     print(scores)\n",
    "#     scores = torch.sigmoid(scores)\n",
    "#     print(scores)\n",
    "    return F.binary_cross_entropy(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score > 0.5, neg_score > 0.5]).numpy()\n",
    "#     print(type(scores))\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "#     scores = F.softmax(scores)\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop goes as follows:\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial does not include evaluation on a validation\n",
    "   set. In practice you should save and evaluate the best model based on\n",
    "   performance on the validation set.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-292-a0bb76d19d18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# in this case, loss will in training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphSAGE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# You can replace DotPredictor with MLPPredictor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLPPredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'dropout'"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "for _ in range(100):\n",
    "    # in this case, loss will in training loop\n",
    "    model = GraphSAGE(train_g.ndata['feat'].shape[1], 16)\n",
    "    # You can replace DotPredictor with MLPPredictor.\n",
    "    pred = MLPPredictor(16)\n",
    "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.001)\n",
    "\n",
    "    # ----------- 4. training -------------------------------- #\n",
    "    all_logits = []\n",
    "    for e in range(80):\n",
    "        # forward\n",
    "        h = model(train_g, train_g.ndata['feat'], mode=\"train\")\n",
    "        pos_score = pred(train_pos_g, h, True)\n",
    "        neg_score = pred(train_neg_g, h, True)\n",
    "        loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pos_score = pred(test_pos_g, h)\n",
    "            neg_score = pred(test_neg_g, h)\n",
    "            val_loss = compute_loss(pos_score, neg_score)\n",
    "#         if val_loss < 0.45 or compute_auc(pos_score, neg_score) > 0.75:\n",
    "#             print('In epoch {}, loss: {}, val_loss: {}, AUC: {}'.format(e, loss, val_loss, compute_auc(pos_score, neg_score)))\n",
    "#             best_model = model\n",
    "#             break\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "    #         print('In epoch {}, loss: {}'.format(e, loss))\n",
    "            print('In epoch {}, loss: {}, val_loss: {}, AUC: {}'.format(e, loss, val_loss, compute_auc(pos_score, neg_score)))\n",
    "\n",
    "\n",
    "# ----------- 5. check results ------------------------ #\n",
    "# def compute_loss(pos_score, neg_score):\n",
    "#     scores = torch.cat([pos_score, neg_score])\n",
    "#     labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "# #     scores = F.sigmoid(scores)\n",
    "#     return F.binary_cross_entropy(scores, labels)\n",
    "\n",
    "# def compute_auc(pos_score, neg_score):\n",
    "#     scores = torch.cat([pos_score > 0.5, neg_score > 0.5]).numpy()\n",
    "# #     print(type(scores))\n",
    "#     labels = torch.cat(\n",
    "#         [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "# #     scores = F.softmax(scores)\n",
    "#     return roc_auc_score(labels, scores)\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    print('AUC', compute_auc(pos_score, neg_score), compute_loss(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 20.0644, -22.6706,  11.8541,   5.1292,   3.2398,  -6.1742, -25.9686,\n",
       "         11.6753, -29.2209,   7.1215], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=3312, num_edges=1505,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 gcn 16 100 subm3_test 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    new_pred, _ = best_model(train_g, train_g.ndata['feat'], new_inf, new_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = pred(new_inf, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res1 = [1 if item > 0.5 else 0 for item in new_pred] \n",
    "with open('ideal.txt', 'w') as f:\n",
    "    for item in li_neww:\n",
    "        f.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0385)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pos_score)[607]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.7167)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(neg_score)[607]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8315), tensor(0.4181))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_score.mean(), neg_score.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
